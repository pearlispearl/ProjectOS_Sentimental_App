# -*- coding: utf-8 -*-
"""Project_OS_Sentimental_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q6p98WdCGA62xm84jQeSL2-AJERWLkpR
"""

import time

start = time.time()

#swifter is a library that intelligently applies operations to Pandas DataFrames,
#automatically choosing the fastest method (vectorized, Dask, or regular Pandas)
#based on the data size and the operation.

!pip install swifter

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import swifter

nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset
try:
    df = pd.read_csv('/content/sentimentdataset.csv')
except FileNotFoundError:
    print("Error: 'sentimentdataset Dataset.csv' not found. Please download it from Kaggle and place it in the current directory.")
    exit()

print("First 5 rows of the dataset:")
df.head()

print("\nDataset shape:")
print(df.shape)
print("\nData types:")
print(df.info())

print("\nSentiment label distribution:")
print(df['Sentiment'].value_counts())

sns.countplot(x='Sentiment', data=df)
plt.title('Sentiment Distribution')
plt.show()

stopwords_set = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub('<[^>]*>', '', text)
    text = re.sub('[^a-zA-Z]', ' ', text)
    text = text.lower()
    tokens = text.split()
    tokens = [word for word in tokens if word not in stopwords_set]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

df['Text'] = df['Text'].swifter.apply(preprocess_text)

print("\nPreprocessed text (first 5 rows):")
print(df['Text'].head())

# !pip install wordcloud

from wordcloud import WordCloud

# Word Cloud for positive post
positive_reviews = ' '.join(df[df['Sentiment'] == 'Positive']['Text'])
wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.title('Word Cloud - Positive Posts')
plt.axis('off')
plt.show()

# Word Cloud for negative post
negative_reviews = ' '.join(df[df['Sentiment'] == 'Negative']['Text'])
wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_reviews)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.title('Word Cloud - Negative Posts')
plt.axis('off')
plt.show()

# Word Cloud for neutral post
neutral_reviews = ' '.join(df[df['Sentiment'] == 'Neutral']['Text'])
wordcloud_neutral = WordCloud(width=800, height=400, background_color='white').generate(neutral_reviews)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_neutral, interpolation='bilinear')
plt.title('Word Cloud - Neutral Post')
plt.axis('off')
plt.show()

# Average review length per sentiment
df['Text_length'] = df['Text'].apply(lambda x: len(x.split()))
avg_length = df.groupby('Sentiment')['Text_length'].mean()
print("\nAverage Text length per sentiment:")
print(avg_length)

sns.boxplot(x='Sentiment', y='Text_length', data=df)
plt.title('Review Length Distribution by Sentiment')
plt.show()

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

vectorizer_bow = CountVectorizer()

X_bow = vectorizer_bow.fit_transform(df['Text'])

df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())

print("Shape of BoW matrix:", X_bow.shape)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer_tfidf = TfidfVectorizer()

X_tfidf = vectorizer_tfidf.fit_transform(df['Text'])

df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())

print("\nShape of TF-IDF matrix:", X_tfidf.shape)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['Sentiment'])

X_bow_train, X_bow_test, y_bow_train, y_bow_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)
X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

print("\nBoW Training set shape:", X_bow_train.shape, y_bow_train.shape)
print("BoW Testing set shape:", X_bow_test.shape, y_bow_test.shape)
print("\nTF-IDF Training set shape:", X_tfidf_train.shape, y_tfidf_train.shape)
print("TF-IDF Testing set shape:", X_tfidf_test.shape, y_tfidf_test.shape)

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score
import joblib  # For saving the model

models = {
    'Naive Bayes (BoW)': MultinomialNB(),
    'Logistic Regression (BoW)': LogisticRegression(max_iter=1000),
    'SVM (BoW)': SVC(kernel='linear', probability=True),
    'Random Forest (BoW)': RandomForestClassifier(n_jobs=-1),
    'Naive Bayes (TF-IDF)': MultinomialNB(),
    'Logistic Regression (TF-IDF)': LogisticRegression(max_iter=1000),
    'SVM (TF-IDF)': SVC(kernel='linear', probability=True),
    'Random Forest (TF-IDF)': RandomForestClassifier(n_jobs=-1)
}

results = {}

for name, model in models.items():
    if 'BoW' in name:
        model.fit(X_bow_train, y_bow_train)
        y_pred = model.predict(X_bow_test)
    else:
        model.fit(X_tfidf_train, y_tfidf_train)
        y_pred = model.predict(X_tfidf_test)

    f1_macro = f1_score(y_bow_test if 'BoW' in name else y_tfidf_test, y_pred, average='macro')
    results[name] = f1_macro
    print(f"{name} - Macro-average F1 Score: {f1_macro:.4f}")

# Find the best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]

print(f"\nBest Model: {best_model_name}")

if 'BoW' in best_model_name:
    joblib.dump((best_model, vectorizer_bow, label_encoder), 'sentiment_model_bow.joblib')
else:
    joblib.dump((best_model, vectorizer_tfidf, label_encoder), 'sentiment_model_tfidf.joblib')

print("\nBest model saved as 'sentiment_model_bow.joblib' or 'sentiment_model_tfidf.joblib'")

# !pip install streamlit
# !streamlit run app_sentiment.py

end = time.time()
print(start)
print(end)
print(end-start)

"""<p style="text-align:center;">That's it! Congratulations! <br>
    Let's now work on your lab assigment.</p>
"""